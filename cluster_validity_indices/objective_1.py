# -*- coding: utf-8 -*-
"""objective_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0u53g4WcYPDbvOPj1s7mizOz_AB6ujn
"""

import numpy as np
import pandas as pd
from copy import deepcopy

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

root_path = 'drive/MyDrive/comment_based_summarization/'  #change dir to your project folder

with open(root_path+'topics_all.txt') as f:
    lines = f.readlines()

files_list = []
for i in range(len(lines)):
  files_list.append(lines[i].replace('\n',''))

for i in range(len(files_list)):
  print('Topic '+str(i+1)+' : '+str(files_list[i]))

import nltk
nltk.download("stopwords")

import gensim.downloader as api
model = api.load('word2vec-google-news-300')

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
import string
import re

def preprocess_sentences(x):
    #Decontraction
    # specific
    x = re.sub(r"won't", "will not", str(x))
    x = re.sub(r"can\'t", "can not", str(x))
    
    # general
    x = re.sub(r"n\'t", " not", str(x))
    x = re.sub(r"\'re", " are", str(x))
    x = re.sub(r"\'s", " is", str(x))
    x = re.sub(r"\'d", " would", str(x))
    x = re.sub(r"\'ll", " will", str(x))
    x = re.sub(r"\'t", " not", str(x))
    x = re.sub(r"\'ve", " have", str(x))
    x = re.sub(r"\'m", " am", str(x))
    
    # Cleaning the urls
    x = re.sub(r'https?://\S+|www\.\S+', '', str(x))

    # Cleaning the html elements
    x = re.sub(r'<.*?>', '', str(x))

    #Removing Special Characters
    x = x.replace('\\r', ' ')
    x = x.replace('\\n', ' ')
    x = x.replace('\\"', ' ')
    x = re.sub('[^A-Za-z0-9]+', ' ', str(x))
    
    # Cleaning the whitespaces
    x = re.sub(r'\s+', ' ', str(x)).strip()
    
    #Removing Punctuation and converting to lower case
    sentence=x.translate(str.maketrans('', '', string.punctuation)).lower()
    word_tokens=sentence.split(' ')
    updated_sentence=""
    #Removing Stopwords
    for w in word_tokens:
        if w not in stop_words:
            updated_sentence+=w+" "
    return updated_sentence

def cal_wmd_bw_doc_com(document_folder,comment_folder):
    
    doc_path = root_path + "processed_documents/"+str(document_folder)+'/'+str(document_folder)
    doc_dir_list = os.listdir(doc_path)
    
    com_path = "C:\\Users\\VISHAL\\Documents\\comment_based_summarization\\processed_comments\\"+str(comment_folder)
    com_dir_list = os.listdir(com_path)
    
    
    document_df=pd.read_csv(doc_path + '.csv')
    
    comment_df=pd.read_csv(com_path + '\\'+ com_dir_list[0])
    
    document_data=deepcopy(document_df)
    document_data['document_sentence'] = document_df.apply(lambda row: preprocess_sentences(row['document_sentence']), axis=1)
    
    comment_data=deepcopy(comment_df)
    comment_data['comment_sentence'] = comment_df.apply(lambda row: preprocess_sentences(row['comment_sentence']), axis=1)
    
    X = document_data['document_sentence'][0]
    
    for i in range(len(document_data['document_sentence'])-1):
        X = " ".join([X, document_data['document_sentence'][i+1]]) 
    
    Y = comment_data['comment_sentence']
    
    comment_wmd = np.zeros((len(Y),))
    print('I am here')
    for j in range(len(Y)):
        comment_wmd[j] = model.wmdistance(X, Y[j])
    
    threshold = 0.4
    
    print('I am here')
    i = 1
    while(True):
        
        print('counter : '+str(i))
        
        length = len(comment_wmd)
        threshold_length = len(comment_wmd[comment_wmd <= threshold])
        
        percent = threshold_length/length
        print('percentage :'+str(percent))
        if percent >= 0.3 and percent <= 0.45:
            break
        
        elif percent <= 0.3:
            threshold = threshold + 0.01
        
        else:
            threshold = threshold - 0.01
        
        i += 1
        
    useful_wmd = np.zeros((len(comment_wmd),), dtype = int)
    for i in range(len(Y)):
        if comment_wmd[i] <= threshold:
            useful_wmd[i] = 1   
    
    data = list(zip(Y, comment_wmd,useful_wmd ))
    df = pd.DataFrame(data=data , columns= ["comment_sentences","word_mover_distance","useful_wmd"])
    
    if not os.path.isdir('C:/Users/VISHAL/Documents/comment based summarization/important_results/'+str(document_folder)):
        os.makedirs('C:/Users/VISHAL/Documents/comment based summarization/important_results/'+str(document_folder))
        
    file_path = root_path + '/important_results/'+str(document_folder)+'/'+ str(document_folder)
    
    df.to_csv(file_path+'_word_mover_distance.csv', index=False)
    
    
    df1 = pd.DataFrame(data=[X] , columns= ["document"])

    df1.to_csv(file_path+'_document_para.csv', index=False)
    
    doc_X = list(document_data['document_sentence'] )
    
    length = len(doc_X)
    document_wmd = []
    
    print('I am here')
    
    for i in range(len(doc_X)):
        temp = []
        for j in range(len(doc_X)):
            if i != j:
                temp.append(model.wmdistance(doc_X[i], doc_X[j]))
        document_wmd.append(temp)
                
    df_wmd = pd.DataFrame(data=document_wmd )

    df_wmd.to_csv(file_path+'_document_wmd.csv', header = False ,index=False)

if __name__ == "__main__":
  for i in range(len(files_list)):
    print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
    cal_wmd_bw_doc_com(files_list[i],files_list[i])
    print('Finished!')

