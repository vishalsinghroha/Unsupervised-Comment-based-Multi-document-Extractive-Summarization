# -*- coding: utf-8 -*-
"""objective_4_rawt_score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7WzCC-8pW6ydcDaIajd6Njo3iDQM13x
"""

import numpy as np
import pandas as pd
from copy import deepcopy

#from keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

import nltk
nltk.download('punkt')
nltk.download('stopwords')

from nltk.tokenize import word_tokenize

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

root_path = 'drive/MyDrive/comment_based_summarization/'  #change dir to your project folder

with open(root_path+'topics_all.txt') as f:
    lines = f.readlines()

files_list = []
for i in range(len(lines)):
  files_list.append(lines[i].replace('\n',''))

for i in range(len(files_list)):
  print('Topic '+str(i+1)+' : '+str(files_list[i]))

dir_folder = root_path+'important_results/'

def calculate_rawt(folder_name):
    
    print('Calculation begins')
    cleaned_doc_df=pd.read_csv(str(dir_folder) + str(folder_name) + '/' + str(folder_name) +'_reader_attention.csv')
    
    complete_document_df = pd.read_csv(str(dir_folder) + str(folder_name) + '/' + str(folder_name) +'_document_para.csv')
    
    ########################################################################################
    #print('\n 1 \n ')
    sentence_raw = []

    empty_sentence = np.zeros( ( len(cleaned_doc_df['reader_attention_of_each_word']),1) ) # contains 0 or 1 , 1 means the sentence is empty 0 means not empty
    
    for i in range(len(cleaned_doc_df['reader_attention_of_each_word'])):
        #print('\n***************************** '+str(i+1) + '**********************************')
        length = len(cleaned_doc_df['reader_attention_of_each_word'][i]) - 2
        #print('Input : '+str(cleaned_doc_df['reader_attention_of_each_word'][i]))
        #print('Length : '+str(length))
        if length != 0:
          temp1 = cleaned_doc_df['reader_attention_of_each_word'][i][1:-2].split(',')
          temp2 = np.asarray(temp1, dtype= float)
          sentence_raw.append(temp2)
          empty_sentence[i] = 0 
        else:
          empty_sentence[i] = 1

    ########################################################################################
    #print('\n 2 \n ')
    complete_sentence = list(complete_document_df['document'])
    
    words_in_sentence = word_tokenize(complete_sentence[0])
    vocabulary_set = set(words_in_sentence)
    vocabulary = list(vocabulary_set)
    print('Size of corpus: '+str(words_in_sentence))
    print('Size of vocabulary: '+str(len(vocabulary)))

    ########################################################################################
    #print('\n 3 \n ')
    X = []
    for i in range(len(cleaned_doc_df['clean_document_sentences'])):
      #print('\n***************************** '+str(i+1) + '**********************************')
      #print('Input : '+str(cleaned_doc_df['clean_document_sentences'][i]))
      #print('Length : '+str(len(cleaned_doc_df['clean_document_sentences'][i])))
      if str(cleaned_doc_df['clean_document_sentences'][i]) != 'nan':
        X.append(cleaned_doc_df['clean_document_sentences'][i])
      
    #for i in range(len(X)):
    #  print('Sentence '+str(i +1) + ' :\n '+ str(X[i]))

    corpus = X.copy()
    #print('After the loop')
    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),('tfid', TfidfTransformer())]).fit(corpus)
    pipe['count'].transform(corpus).toarray()
    #print('Tf-idf shape : '+str(pipe['tfid'].idf_.shape))
    ########################################################################################
    #print('\n 4 \n ')
    sentence_tfidf_score = []
    for i in range(len(corpus)):
        word_tfidf_score = []

        temp_word_tokenizer = word_tokenize(corpus[i])

        for j in range(len(temp_word_tokenizer)):
            try:
              word_index = vocabulary.index(temp_word_tokenizer[j])
              word_tfidf_score.append(pipe['tfid'].idf_[word_index])
            except ValueError:
              word_tfidf_score.append(0)
        sentence_tfidf_score.append(word_tfidf_score)  
        
    ########################################################################################
    gamma = 0.5
    
    sentence_weight = []
    for i in range(len(sentence_raw)):
        word_weight = []
        #print('************************ '+str(i+1)+ '************************')
        #print('sentence_tfidf_score['+str(i+1)+']'+str(sentence_tfidf_score[i]))
        #print('Length of sentence_tfidf_score['+str(i+1)+'] : '+str(len(sentence_tfidf_score[i])))
        
        #print('sentence_raw['+str(i+1)+']'+str(sentence_raw[i]))
        #print('Length of sentence_raw['+str(i+1)+'] : '+str(len(sentence_raw[i])))

        for j in range(min(len(sentence_raw[i]),len(sentence_tfidf_score[i]))):
            #print('j " '+str(j+1))
            
            calculate_weight = (sentence_tfidf_score[i][j] + (gamma*sentence_raw[i][j]) )/(1 + gamma)
            word_weight.append(calculate_weight)

        sentence_weight.append(word_weight)  
    ########################################################################################
    
    #final_sentence_score = np.zeros((len(sentence_weight),1))
    final_sentence_score = np.zeros((len(empty_sentence),1))
    length = len(empty_sentence) 

    index = 0
    for i in range(len(empty_sentence)):
        if empty_sentence[i] == 0:
          final_sentence_score[i] = sum(sentence_weight[index])/length
          index += 1
        else:
          final_sentence_score[i] = 0  
    
    ########################################################################################
    print('Calculation complete')
    df = pd.DataFrame(data= final_sentence_score,columns = ['rawt_score'])
    
    #file_path = 'C:/Users/VISHAL/Documents/comment based summarization/important_results/'+str(folder_name)+'/'+ str(folder_name)
    
    df.to_csv(str(dir_folder) + str(folder_name) + '/' + str(folder_name) +'_rawt_score.csv', index=False)

#sentence_raw = []
#temp3 = np.asarray(0, dtype= float).reshape(-1,1)
#sentence_raw.append(temp3)
#print(sentence_raw[0][0])

i =14
print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
calculate_rawt(files_list[i])
print('Finished!')

for i in range(len(files_list)):
    print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
    calculate_rawt(files_list[i])
    print('Finished!')

