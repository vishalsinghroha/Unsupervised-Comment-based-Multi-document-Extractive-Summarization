# -*- coding: utf-8 -*-
"""objective5_entity_function_version_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uMC6NBB9eZGxtrroqvPIEGctANbGkIA
"""

import numpy as np
import pandas as pd
from copy import deepcopy

#from keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

import nltk
nltk.download('punkt')
nltk.download('stopwords')

from nltk.tokenize import word_tokenize

import spacy.cli
spacy.cli.download("en_core_web_lg")

import spacy
from spacy import displacy
from collections import Counter
import en_core_web_lg
nlp = en_core_web_lg.load()

from google.colab import drive
drive.mount('/content/drive')

root_path = 'drive/MyDrive/comment_based_summarization/'  #change dir to your project folder

with open(root_path+'topics_all.txt') as f:
    lines = f.readlines()

files_list = []
for i in range(len(lines)):
  files_list.append(lines[i].replace('\n',''))

for i in range(len(files_list)):
  print('Topic '+str(i+1)+' : '+str(files_list[i]))

path = 'drive/MyDrive/vae/'  #change dir to your project folder
comment_folder = path + 'text_comments'

document_folder = path + 'text_documents'

doc = nlp("Apple acquired Zoom in China on Wednesday 6th May 2020.\
This news has made Apple and Google stock jump by 5% on Dow Jones Index in the \
United States of America")
print([(X.text, X.label_) for X in doc.ents ])

type(doc.ents)

print([(X, X.ent_iob_, X.ent_type_) for X in doc])

labels = [x.label_ for x in doc.ents]
Counter(labels)

def calculate_entity(file_name):
    
    print('Calculation begins')

    document = open(document_folder + '/' + file_name + '.sent', "r")
    doc_sent = document.readlines()

    comment = open(comment_folder + '/' + file_name + '.cmt', "r")
    comment_sent = comment.readlines()
    
    ################## Finding named entites present in each document sentence and storing them ####################
    doc_entity_list = []
    doc_sentences = ''
    for i in range(len(doc_sent)):
        doc_sentences += doc_sent[i][1:-2]
        temp_doc_entity = nlp(doc_sent[i][1:-2])
        
        sent_entities = []
        
        for entity in temp_doc_entity.ents:
          sent_entities.append(entity.text)

        doc_entity_list.append(sent_entities)

    ################## Finding named entites present in each comment sentence and storing them ####################
    comment_entity_list = []
    print('Length of comment sentences: '+str(len(comment_sent)))
    for i in range(len(comment_sent)):
        temp_comment_entity = nlp(comment_sent[i][1:-2])
        
        sent_entities = []
        
        for entity in temp_comment_entity.ents:
          sent_entities.append(entity.text)

        comment_entity_list.append(sent_entities)

    print('Length of comment sentences entities: '+str(len(comment_entity_list)))
    #print('document sentences:')
    #print(doc_sentences)
     
    ################## Finding all the unique entiies present in the document #################### 
    
    doc = nlp(doc_sentences)

    entities = []

    for entity in doc.ents:
      entities.append(entity.text)

    entity_set = set(entities)

    length_of_unique_entity = len(entity_set)
    print('Number of unique entities: '+str(length_of_unique_entity))

    ################## Computing weight of each document sentence ################################
    weight = np.zeros(len(doc_sent))

    for i in range(len(doc_entity_list)):
      frequency = []
      doc_sentence_entity = doc_entity_list[i]
      
      for j in range(len(doc_sentence_entity)):
        word_frequency = 1 # initial weight given to each word
        for k in range(len(comment_entity_list)):
          comment_sentence_entity = comment_entity_list[k]
          #print('\n ************************* \n Comment Sentence ' +str(k+1) +' entities:\n' + str(comment_entity_list[k]))
          for l in range(len(comment_sentence_entity)):
            if doc_sentence_entity[j] in comment_sentence_entity[l]:
              word_frequency += 1 # calculating frequency of each word
        frequency.append(word_frequency)
      print('Frequency of ' + str(i+1) + ' sentence: '+str(sum(frequency)))  
      weight[i] = sum(frequency)/ length_of_unique_entity

    ################## Saving the outputs of 5 objectives ########################################

    final_path = 'drive/MyDrive/comment_based_summarization/objective_5/'  #change dir to your project folder
    df = pd.DataFrame(data = weight, columns = ['entity_weight'])
    df.to_csv(final_path + file_name + '.csv', index=False)
    #file = open(final_path + file_name + '.entities', 'w')
    #file.write(final_result)
    #file.close()

i =0
print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
calculate_entity(files_list[i])
print('Finished!')

#i =15
#print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
#calculate_weight(files_list[i])
#print('Finished!')

for i in range(len(files_list)):
    print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
    calculate_entity(files_list[i])
    print('Finished!')

