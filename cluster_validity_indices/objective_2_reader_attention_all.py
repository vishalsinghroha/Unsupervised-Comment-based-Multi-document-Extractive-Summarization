# -*- coding: utf-8 -*-
"""objective_2_reader_attention_all.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hk7mnj_QBIy3dgY92S54Hcdqtg41jnFq
"""

import numpy as np
import pandas as pd
from copy import deepcopy

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

root_path = 'drive/MyDrive/comment_based_summarization/'  #change dir to your project folder

with open(root_path+'topics_all.txt') as f:
    lines = f.readlines()

#import os
#files_list = root_path + 'processed_comments'
#comment_dir_list = os.listdir(comment_path)
 
#print("comment Files and directories in '", comment_path, "' :")
 
# prints all files
#print(comment_dir_list)

files_list = []
for i in range(len(lines)):
  files_list.append(lines[i].replace('\n',''))

for i in range(len(files_list)):
  print('Topic '+str(i+1)+' : '+str(files_list[i]))

from copy import deepcopy
import nltk
from nltk.tokenize import word_tokenize
nltk.download("stopwords")
nltk.download('punkt')

import gensim
from gensim.models import KeyedVectors
import gensim.downloader as api
google_path = root_path + 'word2vec-google-news-300.gz'
word2vec = gensim.models.KeyedVectors.load_word2vec_format(google_path, binary=True)

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
import string
import re

def preprocess_sentences(x):
    #Decontraction
    # specific
    x = re.sub(r"won't", "will not", str(x))
    x = re.sub(r"can\'t", "can not", str(x))
    
    # general
    x = re.sub(r"n\'t", " not", str(x))
    x = re.sub(r"\'re", " are", str(x))
    x = re.sub(r"\'s", " is", str(x))
    x = re.sub(r"\'d", " would", str(x))
    x = re.sub(r"\'ll", " will", str(x))
    x = re.sub(r"\'t", " not", str(x))
    x = re.sub(r"\'ve", " have", str(x))
    x = re.sub(r"\'m", " am", str(x))
    
    # Cleaning the urls
    x = re.sub(r'https?://\S+|www\.\S+', '', str(x))

    # Cleaning the html elements
    x = re.sub(r'<.*?>', '', str(x))

    #Removing Special Characters
    x = x.replace('\\r', ' ')
    x = x.replace('\\n', ' ')
    x = x.replace('\\"', ' ')
    x = re.sub('[^A-Za-z0-9]+', ' ', str(x))
    
    # Cleaning the whitespaces
    x = re.sub(r'\s+', ' ', str(x)).strip()
    
    #Removing Punctuation and converting to lower case
    sentence=x.translate(str.maketrans('', '', string.punctuation)).lower()
    word_tokens=sentence.split(' ')
    updated_sentence=""
    #Removing Stopwords
    for w in word_tokens:
        if w not in stop_words:
            updated_sentence+=w+" "
    return updated_sentence

def calculate_reader_attention(document_df,comment_file,wmd_file):
    
    #doc_path = "C:\\Users\\VISHAL\\Documents\\comment based summarization\\processed_documents\\"+str(document_folder)
    #doc_dir_list = os.listdir(doc_path)
    
    #com_path = "C:\\Users\\VISHAL\\Documents\\comment based summarization\\processed_comments\\"+str(comment_folder)
    #com_dir_list = os.listdir(com_path)
    
    #word_mover_dist_path = "C:\\Users\\VISHAL\\Documents\\comment based summarization\\important_results\\"+str(wmd_folder)
    #word_mover_dist_dir_list = os.listdir(word_mover_dist_path)
    
    #document_df = pd.read_csv(document_file + '.csv')
    
    comment_df = pd.read_csv(comment_file + '.csv')
    
    wmd_df = pd.read_csv(wmd_file + '_word_mover_distance.csv')
    
    #################################################################################################
    
    document_data=deepcopy(document_df)
    document_data['document_sentence'] = document_df.apply(lambda row: preprocess_sentences(row['document_sentence']), axis=1)
    
    comment_data=deepcopy(comment_df)
    comment_data['comment_sentence'] = comment_df.apply(lambda row: preprocess_sentences(row['comment_sentence']), axis=1)
    
    ################################################################################################
    X = document_data['document_sentence']
    comment_words = []
    Y = comment_data['comment_sentence']
    
    for sentence in Y:
        comment_words.append(word_tokenize(sentence)) 
    
    ################################################################################################
    clean_document_sentences = []

    document_vector = []

    for sentence in X:
        temp_tokenize = word_tokenize(sentence)
        final_sentence_words = []
        for i in range(len(temp_tokenize)):  
            try:
                document_vector.append(word2vec[temp_tokenize[i]])
                final_sentence_words.append(temp_tokenize[i])
            except KeyError:
                pass
            
        clean_document_sentences.append(final_sentence_words)    
    
    ################################################################################################
    sentence_length = np.zeros((len(clean_document_sentences),),dtype=int)
    
    for i in range(len(clean_document_sentences)):
        sentence_length[i] = len(clean_document_sentences[i])
        
    ################################################################################################
    print('I am before encoding')
    comment_vector = []

    for i in range(len(comment_words)):
        temp_comment_vector = []

        for j in range(len(comment_words[i])):
            try:
                temp_comment_vector.append(word2vec[comment_words[i][j]])
            except KeyError:
                pass

        comment_vector.append(temp_comment_vector)
    print('I am after encoding')
    ##################################################################
    temp_max_allignment_score = []
    print('Length of document vector: '+str(len(document_vector)))
    allignment_score = []
    counter = 0
    for i in range(len(document_vector)):
        counter += 1
        if (counter)%500 == 0:
          print('Iteration no: '+str(i+1))
        
        for j in range(len(comment_vector)):
            temp_allignment_score = []
            for k in range(len(comment_vector[j])):
                temp_allignment_score.append(np.dot(np.transpose(document_vector[i].reshape(300,1)) ,comment_vector[j][k].reshape(300,1)))
               

            allignment_score.append(temp_allignment_score)
 
    print('Calculation complete')
    ##############################################################################
    for i in range(len(allignment_score)):      
        if len(allignment_score[i]) != 0:
            temp_max_allignment_score.append(max(allignment_score[i]))
        else:    
            temp_max_allignment_score.append(0)
    ##############################################################################
    max_allignment_score = np.zeros((len(comment_vector),len(document_vector)))
    index = 0
    for i in range(len(comment_vector)):  
        for j in range(len(document_vector)):  
            max_allignment_score[i][j] = temp_max_allignment_score[index]    
            index += 1    
    
    ################################################################################
    salience_score = np.zeros(len(wmd_df['useful_wmd']),dtype=int)

    for i in range(len(wmd_df['useful_wmd'])):
        salience_score[i] = wmd_df['useful_wmd'][i]
    
    #################################################################################
    
    temp_epislon = 0.0
    for j in range(len(max_allignment_score)):  
        temp_epislon = temp_epislon + np.multiply(max_allignment_score[j],salience_score[j]) 
    
    ################################################################################
    epsilon = np.exp(temp_epislon)/ np.sum(np.exp(temp_epislon))
    
    ################################################################################
    
    final_reader_attention = []
    counter = 0
    for i in range(len(clean_document_sentences)):
        temp_reader_attention = []
        for j in range(len(clean_document_sentences[i])):
            temp_reader_attention.append(epsilon[counter])
            counter += 1

        final_reader_attention.append(temp_reader_attention)
        
    ################################################################################
    
    average_sentence_attention = np.zeros((len(final_reader_attention),))

    for i in range(len(final_reader_attention)):
        if len(final_reader_attention[i]) != 0:
            average_sentence_attention[i] = sum(final_reader_attention[i])/len(final_reader_attention[i])
        else:
            average_sentence_attention[i] = 0    
        
    ################################################################################
    
    final_clean_document_sentences = []

    for i in range(len(clean_document_sentences)):
        if len(clean_document_sentences[i]) != 0:
            temp = clean_document_sentences[i][0]
            for j in range(len(clean_document_sentences[i])-1):
                temp = "{in1} {in2}".format(in1=temp, in2=clean_document_sentences[i][j+1])

            final_clean_document_sentences.append(temp)
            temp =''
        else:
            final_clean_document_sentences.append([''])
    
    ################################################################################
    
    data = list(zip(final_clean_document_sentences,final_reader_attention, average_sentence_attention,sentence_length ))
    df = pd.DataFrame(data= data, columns= ["clean_document_sentences","reader_attention_of_each_word","average_reader_attention","sentence_length"])
    #wmd_folder = root_path+'important_results/'
    #file_path = 'C:/Users/VISHAL/Documents/comment based summarization/important_results/'+str(document_folder)+'/'+ str(document_folder)
    
    df.to_csv(wmd_file + '_reader_attention_part2.csv', index=False)

if __name__ == "__main__":
  comment_folder = root_path+'processed_comments/'
  document_folder = root_path+'processed_documents/'
  wmd_folder = root_path+'important_results/'

  for i in range(len(files_list)):
    print('\n ******************************* \n Topic No.: '+str(i+1)+'\n Topic Name: '+str(files_list[i]))
    calculate_reader_attention(document_folder + files_list[i]+'/'+files_list[i],comment_folder + files_list[i] +'/'+files_list[i], wmd_folder + files_list[i]+'/'+files_list[i] )
    print('Finished!')
